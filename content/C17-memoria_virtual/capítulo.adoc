= Paginación bajo demanda
ifndef::sectiondir[:sectiondir: .]
:imagesdir: {sectiondir}/images
include::../../config/attributes.adoc[]

La _paginación bajo demanda_ es la técnica con la que frecuentemente se implementa la _memoria virtual_ en los sistemas con paginación.
El concepto de _memoria virtual_ no debe confundirse con el de _espacio de direcciones virtual_, aunque están relacionados puesto que _el que exista separación entre la memoria física y la manera en la que los procesos perciben la memoria es un requisito para poder implementar la memoria virtual_.

== Memoria virtual

_La *memoria virtual* es una técnica que permite la ejecución de procesos sin que éstos tengan que ser cargados completamente en la memoria_.

Los programas suelen tener partes de código que rara vez son ejecutadas, por ejemplo las funciones para manejar condiciones de error que, aunque útiles, generalmente nunca son invocadas.
También es frecuente que se reserve más memoria para datos de lo que realmente es necesario.
Por ejemplo muchos programadores tiene la costumbres de hacer cosas tales como declarar un _array_ de 1000 por 1000 elementos cuando realmente sólo necesitan 100 por 100.
Teniendo todo esto en cuenta y con el fin de mejorar el aprovechamiento de la memoria, parece que sería interesante no tener que cargar todas las porciones de los procesos pero de manera que éstos aun así puedan seguir siendo ejecutados.
Eso es exactamente lo que proporciona la memoria virtual, en general, y la paginación bajo demanda, en particular, para los sistemas que soportan paginación.

La habilidad de ejecutar un proceso cargado parcialmente en memoria proporciona algunos beneficios importantes:

* _Un programa no estará nunca más limitado por la cantidad de memoria disponible_.
Es decir, los desarrolladores pueden escribir programas considerando que disponen de un espacio de direcciones virtual extremadamente grande y sin considerar la cantidad de memoria realmente disponible.
Es importante no olvidar que sin memoria virtual para que un proceso pueda ser ejecutado debe estar completamente cargado en la memoria.

* Puesto que cada programa ocupa menos memoria _más programas se pueden ejecutar al mismo tiempo, con el correspondiente incremento en el uso de la CPU y en el rendimiento del sistema_ y sin efectos negativos en el tiempo de respuesta y en el de ejecución.

== Método básico

_En la paginación bajo demanda las páginas individuales, en las que se dividen los espacios de direcciones virtuales de los diferentes procesos, pueden ser sacadas de la memoria de manera temporal y copiadas a un almacenamiento de respaldo, para posteriormente volver a ser traídas a la memoria cuando son necesitadas por su proceso_.
A este proceso de guardado y recuperación de las páginas sobre el almacenamiento de respaldo se lo denomina *intercambio* o _swapping_ y es llevado a cabo por un componente del sistema operativo denominado el _paginador_.

Para que se puedan cargar las páginas cuando son necesitadas por su proceso hace falta que el paginador sepa cuando lo son.
Eso requiere que el hardware proporcione algún tipo de soporte, por ejemplo incorporando un *bit de válido* a la entrada de cada página en la tabla de páginas:

* _Cuando el bit de válido está a 1 la página es legal y está en la memoria_.
Es decir, la página existe en el espacio de direcciones virtual del proceso y tiene asignado un marco de memoria física.

* _Cuando el bit de válido está a 0_ pueden ocurrir varias cosas:

    ** _La página es legal pero esta almacenada en disco_ y no en la memoria.

    ** _La página no es legal_.
    Es decir, no existe en el espacio de direcciones virtual del proceso.
    Esto puede ser debido a que la página esté en un hueco del espacio de direcciones —en una región que no está siendo utilizada— por lo que el sistema operativo no le ha asignado espacio de almacenamiento ni en disco ni en la memoria.

Si un proceso accede a una página _residente en memoria_ —marcada como válida— no ocurre nada y la instrucción se ejecuta con normalidad.
Pero si accede a una página marcada como inválida:

. Al intentar acceder a la página la MMU comprueba el bit de válido y _genera una excepción de fallo página al estar marcada como inválida_.
Dicha excepción es capturada por el sistema operativo.

. _El sistema operativo comprueba en una tabla interna si la página es legal o no_.
Es decir, si la página realmente no pertenece al espacio de direcciones virtual del proceso o si pertenece pero está almacenada en el disco.
Esta tabla interna suele almacenarse en el PCB del proceso como parte de la información de gestión de la memoria.

. _Si la página es ilegal, el proceso ha cometido un error y debe ser terminado_.
En UNIX, por ejemplo, el sistema envía al proceso una señal de _violación de segmento_ que lo obliga a terminar.

. _Si la página es legal debe ser cargada desde el disco_:

..
_El núcleo debe buscar un marco de memoria libre_ que, por ejemplo, se puede escoger de la lista de marcos libres del sistema.

..
_Se solicita una operación de disco para leer la página deseada en el marco asignado_.
Puesto que no resulta eficiente mantener la CPU ocupada mientras la página es recuperada desde el disco, el sistema debe solicitar la lectura de la página y poner al proceso en estado de espera.

..
_Cuando la lectura del disco haya terminado se debe modificar la tabla interna, antes mencionada, y la tabla de páginas para indicar que la página está en la memoria_.

..
_Reiniciar la instrucción que fue interrumpida por la excepción_.
Generalmente esto se hace colocando el proceso nuevamente en la cola de preparados y dejando que el asignador lo reinicie cuando sea escogido por el planificador de la CPU.

Un caso extremo de la paginación bajo demanda es la *paginación bajo demanda pura*.
En ella _la ejecución de un proceso se inicia sin cargar ninguna página en la memoria_.
Cuando el sistema operativo sitúa al contador de programas en la primera instrucción del proceso —que es una página no residente en memoria— se genera inmediatamente un fallo de página.
La página es cargada en la memoria —tal y como hemos descrito anteriormente— y el proceso continua ejecutándose, fallando cuando sea necesario con cada página que necesite y no esté cargada.
Las principales ventajas de la _paginación bajo demanda pura_ son:

* Nunca se traerá desde el disco una página que no sea necesaria.

* El inicio de la ejecución de un proceso es mucho más rápido que si se cargara todo el proceso desde el principio.

== Requerimientos de la paginación bajo demanda

Los requerimientos hardware para que un sistema operativo pueda soportar la paginación bajo demanda son:

* _Tabla de páginas con habilidad para marcar entradas inválidas_, ya sea utilizando un bit específico o con valores especiales en los bits de protección.

* _Disponibilidad de una memoria secundaria_.
En esta memoria se guardan las páginas que no están presentes en la memoria principal.
Normalmente se trata de un disco conocido como *dispositivo de intercambio*, mientras que la sección de disco utilizada concretamente para dicho propósito se conoce como *espacio de intercambio* o _swap_.

* _Posibilidad de reiniciar cualquier instrucción_ después de un fallo de página.
En la mayor parte de los casos esta funcionalidad es sencilla de conseguir.
Sin embargo, la mayor dificultad proviene de las instrucciones que pueden modificar diferentes posiciones de la memoria, como aquellas pensadas para mover bloques de bytes o palabras.
En el caso de que el bloque de origen o de destino atraviese un borde de página, la instrucción sería interrumpida cuando la operación solo haya sido realizada parcialmente.
Si además ambos bloques se superpusieran, no se podría reiniciar la instrucción completa.
Las posibles soluciones a este problema deben ser implementadas en el hardware.

== Rendimiento de la paginación bajo demanda

Indudablemente el rendimiento de un sistema con paginación bajo demanda se ve afectado por el número de fallos de páginas.
En el peor de los casos, en cada instrucción un proceso puede intentar acceder a una página distinta empeorando notablemente el rendimiento.
Sin embargo esto no ocurre puesto que los programas tienden a tener localidad de referencia (véase el <<_hiperpaginación>>).

=== Tiempo de acceso efectivo

El rendimiento de un sistema con paginación bajo demanda está relacionado con el concepto de *tiempo de acceso efectivo* a la memoria.
_Éste intenta estimar el tiempo que realmente se tarda en acceder a la memoria teniendo en cuenta mecanismos del sistema operativo como la paginación bajo demanda_.

En muchos sistemas informáticos el *tiempo de acceso* —a la memoria física— stem:[T_m] es de unos pocos nanosegundos.
Por lo tanto, si no hay fallos de página, el _tiempo de acceso efectivo_ es igual al tiempo de acceso a la memoria.
Pero si hay fallos de página, primero es necesario leer la página del disco, por lo que el _tiempo de acceso efectivo_ a la memoria es mayor.

Supongamos que conocemos la probabilidad stem:[p] de que ocurra un fallo de página.
_El tiempo de acceso efectivo se podría calcular como una media ponderada por la probabilidad p del tiempo de acceso a la memoria stem:[T_m] mas el tiempo necesario para gestionar cada fallo de página —o *tiempo de fallo de página*— stem:[T_(fp)]:

[stem]
++++
T_(em)=(1-p)*T_m+p T_(fp)
++++

Por tanto, para calcular el _tiempo de acceso efectivo_ stem:[T_(em)] necesitamos estimar el _tiempo de fallo de página_ stem:[T_(fp)], que se consume fundamentalmente en:

* _Servir la excepción de fallo de página_.
Esto incluye capturar la interrupción, salvar los registros y el estado del proceso, determinar que la interrupción es debida a una excepción de fallo de página, comprobar si la página es legal y determinar la localización de la misma en el disco.
Aproximadamente, en realizar esta tarea el sistema puede tardar de 1 a 100μs.

* _Leer la página en un marco libre_.
En esta tarea se puede tardar alrededor de 8ms, pero este tiempo puede ser mucho mayor si el dispositivo está ocupado y se debe esperar a que se realicen otras operaciones.

* _Reiniciar el proceso_.
Si incluimos el tiempo de espera en la cola de preparados, se puede tardar entre 1 y 100μs.

Como se puede apreciar _la mayor parte del tiempo de fallo de página es debido al tiempo requerido para acceder al dispositivo de intercambio_.

Para ilustrar el cálculo del _tiempo de acceso efectivo_ a la memoria: sólo vamos a considerar el tiempo requerido para acceder al dispositivo de intercambio —ignorando las otras tareas a realizar durante el fallo de página— vamos suponer que el _tiempo de acceso_ a la memoria stem:[T_m] es de 200 ns y que la probabilidad stem:[p] es muy pequeña (es decir, stem:[p ≪ 1]):

[stem]
++++
{:(T_(em),=,(1-p)*200ns+p * 8ms),
(      ,=,(1-p)*200ns+p * 8000000ns),
( ,approx, 200ns+7999800ns * p ):}
++++

Como se puede apreciar el _tiempo de acceso efectivo_ es proporcional a la *tasa de fallos de página*.

 [stem]
 ++++
 T_(em) approx T_(m)+r_(fp)
 ++++

Por ejemplo, si un proceso causa un fallo de página en uno de cada 1000 accesos (stem:[p = 0,001]), el _tiempo de acceso efectivo_ es de 8,2 ms.
Es decir, el rendimiento del sistema es 40 veces inferior debido a la paginación bajo demanda.
Por tanto _es necesario mantener la tasa de fallos de página lo más baja posible para mantener un rendimiento adecuado_.

=== Manejo y uso del espacio de intercambio

_Otro aspecto fundamental que afecta al rendimiento de la paginación bajo demanda es el uso del espacio de intercambio_.
Cuando un proceso genera un fallo de página el sistema operativo debe recuperar la página de allí donde esté almacenada.
Si esto ocurre al principio de la ejecución, ese lugar seguramente será el archivo que contiene la imagen binara del programa, pues es donde se encuentran las páginas en su estado inicial.
Sin embargo el acceso al espacio de intercambio es mucho más eficiente que el acceso a un sistema de archivos, incluso aunque el primero esté almacenado dentro de un archivo de gran tamaño.
Esto es debido a que los datos se organizan en bloques contiguos de gran tamaño, se evitan las búsquedas de archivos y las indirecciones en la asignación de espacio.
Por ello debemos plantearnos que hacer con las imágenes de los programas que van a ser ejecutados.

* Se puede mejorar el rendimiento _copiando en el espacio de intercambio la imagen completa de los programas durante el inicio del proceso, para después realizar la paginación bajo demanda sobre dicha copia_.

* Otra alternativa es _cargar las páginas desde el archivo que contiene la imagen cuando son usadas por primera vez pero siendo escritas en el espacio de intercambio cuando dichas páginas tiene que ser reemplazadas_.
Esta aproximación garantiza que sólo las páginas necesarias son leídas desde el sistema de archivos reduciendo el uso de espacio de intercambio, mientras que las siguientes operaciones de intercambio se hacen sobre dicho espacio.

* También se puede suponer que el código de los procesos no puede cambiar.
Esto permite _utilizar el archivo de la imagen binaria para recargar las páginas de código, lo que también evita escribirlas cuando son sustituidas.
Sin embargo el espacio de intercambio se sigue utilizando para las páginas que no están directamente asociadas a un archivo, como la pila o el montón de los procesos_.
Este método parece conseguir un buen compromiso entre el tamaño del espacio de intercambio y el rendimiento.
Por eso se utiliza en la mayor parte de los sistemas operativos modernos.

== Copy-on-write

_El *copy-on-write* o copia durante la escritura permite la creación rápida de nuevos procesos, minimizando la cantidad de páginas que deben ser asignadas a estos_.
Para entenderlo es importante recordar que la llamada al sistema `fork()` crear un proceso hijo cuyo espacio de direcciones es un duplicado del espacio de direcciones del padre.
Indudablemente esto significa que durante la llamada es necesario asignar suficientes marcos de memoria física como para alojar las páginas del nuevo proceso hijo.
El _copy-on-write_ minimiza de la siguiente manera el número de marcos que deben ser asignadas al nuevo proceso:

. _Cuando la llamada al sistema `fork()` crea el nuevo proceso lo hace de forma que éste comparta todas sus páginas con las del padre_ (véase la ).
Sin el _copy-on-write_ el `fork()` tendría que asignar marcos de memoria física a el hijo, para a continuación copiar las páginas del padre en ellos.
Sin embargo con el _copy-on-write_ padre e hijo mapean sus páginas en los mismos marcos, evitando tener que asignar memoria libre.

. _Las páginas compartidas se marcan como copy-on-write_.
Para ello se puede marcar todas las páginas como de _solo lectura_ en la tabla de páginas de ambos procesos y utilizar una tabla interna alojada en el PCB para indicar cuales son realmente de _sólo lectura_ y cuales están en _copy-on-write_.
_Es importante destacar que realmente sólo las páginas que pueden ser modificadas se marcan como copy-on-write._ Las páginas que no puede ser modificadas —por ejemplo las que contienen el código ejecutable del programa— simplemente pueden ser compartidas como de sólo lectura por los procesos, como hemos comentado anteriormente.

. _Si algún proceso intenta escribir en una página copy-on-write, la MMU genera una excepción para notificar el suceso al sistema operativo_.
Siguiendo lo indicado en el punto anterior, la excepción se originaría porque la página está marcada como de _solo lectura,_ por lo que el sistema operativo debería comprobar si se trata de un acceso a una página _copy-on-write_ o a un intento real de escribir en una página de _sólo lectura_.
Para ello el sistema sólo tendría que mirar la tabla interna almacenada en el PCB.
Si se ha intentado escribir en una página de _solo lectura_, el proceso ha cometido un error y generalmente debe ser terminado.

.  _Si el sistema detecta una escritura a una página de copy-on-write sólo tiene que copiarla en un marco libre y mapearlo en el espacio de direcciones del proceso_ (véase la ).
Para esto se sustituye la página compartida por otra que contiene una copia pero que ya no está compartida.
Indudablemente la nueva página debe ser marcada como de escritura para que en el futuro pueda ser modificada por el proceso.

.  _La página original marcada como copy-on-write puede ser marcada como de escritura y no como copy-on-write, pero sólo si ya no va a seguir siendo compartida_.
Esto es así porque una página marcada como _copy-on-write_ puede ser compartida por varios procesos.

.  _El sistema operativo puede reiniciar el proceso_.
A partir de ahora éste puede escribir en la página sin afectar al resto de los procesos.
Sin embargo puede seguir compartiendo otras páginas en _copy-on-write_.

El _copy-on-write_ permite ahorrar memoria y tiempo en la creación de los procesos puesto que sólo se copian las páginas que son modificadas por éstos, por lo que se trata de una técnica común en múltiples sistemas operativos, como por ejemplo Microsoft Windows, Linux y Solaris.

El _copy-on-write_ _es especialmente interesante si a continuación se va a utilizar la llamada al sistema exec() puesto que si es así copiar el espacio de direcciones completo es una pérdida de tiempo_.

== Archivos mapeados en memoria

_Los *archivos mapeados en memoria* permiten acceder a un archivo como parte del espacio de direcciones virtuales de un proceso_.
Algunas de las características de esta técnica son:

* Cuando una región del espacio de direcciones queda marcada para ser mapeada sobre una región de un archivo _se utiliza una estrategia similar a la comentada para el método básico de la paginación bajo demanda.
La diferencia es que las páginas son cargadas desde dicho archivo y no desde el espacio de intercambio_.
Es decir, en un primer acceso a una página mapeada se produce un fallo de página que es resuelto por el sistema operativo leyendo una porción del archivo en el marco asignado a la página.

* Esto significa que _la lectura y escritura del archivo se realiza a través de lecturas y escrituras en la memoria_, lo que simplifica el acceso y elimina el costo adicional de las llamadas al sistema: `read()`, `write()`, etc.

* _Las escrituras en disco se suelen realizar de forma asíncrona_.
Para ello el sistema operativo comprueba periódicamente las páginas modificadas y las escribe en disco.

* _Los marcos utilizados en el mapeo pueden ser compartidos, lo que permite compartir los datos de los archivo_.
Además se puede incluir soporte de _copy-on-write_, lo que permite a los procesos compartir un archivo en modo de sólo lectura pero disponiendo de sus propias copias de aquellas páginas que modifiquen.
Indudablemente para que los procesos puedan compartir datos es necesario que exista algún tipo de coordinación (véase el <<_sincronización>>).

Algunos sistemas operativos ofrecen el servicio de mapeo de archivos en la memoria sólo a través de una llamada al sistema concreta, permitiendo utilizar las llamadas estándar —`read()`, `write()`, etc.— para hacer uso de la E/S tradicional.
Sin embargo _muchos sistemas modernos utilizan el mapeo en la memoria independientemente de que se pidan o no_.
Por ejemplo, en Linux si un proceso utiliza llamada al sistema `mmap()` es porque explícitamente pide que el archivo sea mapeado en memoria.
Por tanto, el núcleo mapea el archivo en el espacio de direcciones del proceso.
Sin embargo, si un archivo es abierto con llamadas al sistemas estándar —como `open()`— Linux mapea el archivo en el espacio de direcciones del núcleo y traduce las llamadas `read()` y `write()` en accesos a la memoria en dicha región.
No importa como sea abierto el archivo, Linux trata toda la E/S a archivos como mapeada en memoria, permitiendo que el acceso a los mismos tenga lugar a través del eficiente componente de gestión de la memoria.

== Reemplazo de página

Hasta el momento hemos considerado que disponemos de memoria física suficiente para atender cualquier fallo de página pero ¿qué pasa cuando no quedan marcos libres?.
En ese caso el código que da servicio a la excepción de fallo de página debe escoger alguna página, intercambiarla con el disco y utilizar el marco de la misma para cargar la nueva página.
Es decir, debemos modificar la función que ejecuta los pasos descritos en el <<_protección_y_seguridad>> de la siguiente manera:

["arabic", start=4] .
. _Si la página es legal, debe ser cargada desde el disco_.

    .. _Buscar la localización de la página en disco_.

    .. _El núcleo debe buscar un marco de memoria libre_ que, por ejemplo, se puede escoger de la lista de marcos libres del sistema.

        ... _Si hay uno, usarlo_.

        ... _Si no hay, usar un algoritmo de reemplazo de página para seleccionar una víctima_.

        ... _Escribir la víctima en el disco_ y cambiar las tablas de paginas y de marcos libres de acuerdo a la nueva situación.
Para evitar mantener la CPU ocupada, el sistema debe solicitar la escritura de la página y poner al proceso en estado de espera.

    .. _Se solicita una operación de disco para leer la página deseada en el marco asignado_.
Para evitar mantener la CPU ocupada, el sistema debe solicitar la escritura de la página y poner al proceso en estado de espera.

    .. _Cuando la lectura del disco haya terminado se debe modificar la tabla interna de páginas válidas y la tabla de páginas para indicar que la página está en la memoria._

    .. _Reiniciar el proceso interrumpido_.

Es importante destacar que _en caso de reemplazo se necesita realizar dos accesos al disco.
Esto se puede evitar utilizando un *bit de modificado* asociado a cada página en la tabla de páginas_.

* _Este bit es puesto a 1 por el hardware cuando se modifica la página_.

* _Se puede evitar escribir en disco aquellas páginas que tienen este bit a 0 cuando son seleccionada para reemplazo_, siempre que el contenido de la página no haya sido sobrescrito por otra en el espacio de intercambio

En general para implementar la paginación bajo demanda necesitamos:

* _Un algoritmo de asignación de marcos_ que se encarga de asignar los marcos a los procesos.

* _Un algoritmo de reemplazo de página_ para seleccionar que página reemplazamos cuando no hay marcos suficientes.

Obviamente estos algoritmos deben ser escogidos de forma que mantengan la tasa de fallos de página lo más baja posible.

// TODO: Recuperar los ejemplos de problemas
=== Algoritmos de reemplazo de páginas

Existen diversos criterios para escoger la página que reemplazamos cuando no hay suficientes marcos disponibles.
En cualquier caso _el algoritmo óptimo —el que garantiza la tasa de fallos de página más baja— consiste en seleccionar siempre la página que más se va a tardar en necesitar_.
Desafortunadamente _este algoritmo es difícil de implementar puesto que necesita tener información acerca de cuáles van a ser las páginas referencias en el futuro_.
Por eso sólo se puede utilizar en estudios comparativos con el fin de saber cuanto se aproxima al óptimo un algoritmo de reemplazo concreto.

Otros algoritmos de reemplazo pueden utilizar uno o varios de los siguientes criterios:

* _Reemplazar la página que hace más tiempo que fue cargada_.
Este criterio da lugar al algoritmo FIFO de reemplazo que no siempre tiene un buen rendimiento puesto que la página más antigua no necesariamente es la que se va a tardar más tiempo en necesitar —que sería la elección óptima—.

* _Reemplazar la página que hace más tiempo que fue utilizada_ bajo la hipótesis de que si una página no ha sido usada durante un gran periodo de tiempo, entonces es poco probable que vaya a serlo en el futuro.
Este criterio da lugar a la familia de algoritmos *LRU* (Least Recently Used):

    ** _Estos algoritmos requieren de soporte por parte del hardware_ puesto que al sistema operativo no se le notifican los acceso legales a las páginas, por lo que no tiene forma de saber cuando una página fue usada por última vez.

    ** _Normalmente el soporte por parte del hardware es a través de un bit en la tabla de páginas llamado **bit de referencia**_.
Este bit se pone a 1 cada vez que una instrucción ejecutada en la CPU referencia a una página, lo que permite al sistema operativo hacerse una idea aproximadafootnote:[Se trata de una aproximación puesto que usando el _bit de referencia_ el sistema operativo no puede conocer con exactitud la última vez que una página fue utilizada.
Sin embargo, aunque existen soluciones exactas que hacen uso de un contador o de una pila que se actualiza en cada acceso a las páginas, se trata de soluciones muy costosas como para ser implementarlas en hardware.] de las páginas que han sido usadas recientemente.
A los algoritmos que siguen esta aproximación se los denomina _*NRU*_ (Not Recently Used).

    ** _Dentro de los algoritmos NRU también están aquellos que son mejorados incluyendo el valor del bit de modificado en el criterio de elección de la página_.
Estos algoritmos escogen las páginas no referencias antes que las referencias —para lo que utilizan el valor del bit de referencia— y dentro de cada clase las no modificadas antes que las modificadas —para lo que utilizan el valor del bit de modificado— para evitar en lo posible reemplazar páginas cuyo contenido tiene que ser escrito en disco.

* _Reemplazar la página que ha sido usada con mayor o menos frecuencia_ utilizando contadores de referencias para cada página —almacenados en la tabla de páginas— que sos actualizados por el hardware en cada referencia.
Este criterio da lugar a los algoritmos *LFU* (Least Frequently Used) —cuando se escogen las páginas utilizadas con menos frecuencia— y *MFU* (Most Frequently Used) —cuando se escogen las páginas utilizadas con más frecuencia—.

=== Algoritmos de buffering de páginas

Existen otros procedimientos que pueden ser utilizados, junto con alguno de los algoritmos de reemplazo comentados, con el objetivo de mejorar su eficiencia.
Estos procedimientos se agrupan dentro de lo que se denomina algoritmos de *buffering de páginas*.

* _Se puede mantener una lista de marcos libres_.
Cuando se produce un fallo de paginas se escoge un marco de la lista y se carga la página, al tiempo que se selecciona una página como víctima y se copia al disco.
Esto permite que el proceso se reinicie lo antes posible, sin esperar a que la página reemplazada sea escrito en el disco.
Posteriormente, cuando la escritura finalice, el marco es incluido en la lista de marcos libres.

* Una mejora de lo anterior sería _recordar que página estuvo en cada marco antes de que éste pasara a la lista de marcos libres_.
De esta forma las páginas podrían ser recuperadas directamente desde la lista si fallara alguna antes de que su marco fuera utilizado por otra página.
Esto permite reducir los efectos de que el algoritmo de reemplazo escoja una víctima equivocada.

* _Se puede mantener una lista de páginas modificadas e ir escribiéndolas cuando el dispositivo del espacio de intercambio no esté ocupado_.
Este esquema aumenta la probabilidad de que una página esté limpia cuando sea seleccionada por el algoritmo de reemplazo, evitando la escritura en disco.

=== Reemplazo local frente a global

Cuando un proceso necesita un marco el algoritmo de reemplazo puede tanto extraerlo de cualquier proceso como ser obligado a considerar sólo aquellas páginas que pertenecen al proceso que generó el fallo.
Eso permite clasificar los algoritmos de reemplazo en dos categorías:

* En _el *reemplazo local* sólo se pueden escoger marcos de entre los asignados al proceso_.

    ** El número de marcos asignados a un proceso no cambia por que ocurran fallos de páginas.

    ** El mayor inconveniente es que _un proceso no puede hacer disponible a otros procesos los marcos de memoria que menos usa_.

* En _el *reemplazo global* se pueden escoger marcos de entre todos los del sistema_, independientemente de que estén asignados a otro proceso o no.

    ** El número de marcos asignados a un proceso puede aumentar si durante los fallos de página se seleccionan marcos de otros procesos.

    ** El mayor inconveniente _es que los procesos no pueden controlar su tasa de fallos de página_, puesto que esta depende del comportamiento de los otros procesos, afectando al tiempo de ejecución de forma significativa.

Generalmente _el reemplazo global proporciona mayor rendimiento por lo que es el método más utilizado_.

== Asignación de marcos de página

La cuestión que queda por resolver es cómo repartir los marcos de memoria física libre entre los diferentes procesos con el fin de cubrir las necesidades de reemplazo de cada uno de ellos.
Posibles soluciones a esto serían: repartir la memoria por igual entre todos los procesos o hacerlo en proporción a la cantidad de memoria virtual que utilizan.
Sin embargo parece que puede ser interesante determinar el mínimo número de marcos que realmente necesita cada proceso, pues así el sistema podría disponer de memoria libre para aumentar el número de procesos —aumentando el uso de la CPU— o para dedicarla a otras funciones —como es el caso de los _búferes_ y las cachés de E/S —.

El mínimo número de marcos viene establecido por diversos factores:

* Cuando ocurre un fallo de página la instrucción que la ha provocado debe ser reiniciada después de cargar la página en un marco libre.
Por lo tanto _un proceso debe disponer de suficientes marcos como para guardar todas las páginas a las que una simple instrucción pueda acceder_, pues de lo contrario el proceso nunca podría ser reiniciado al fallar permanentemente en alguno de los acceso a memoria de la instrucción.
Obviamente este límite viene establecido por la arquitectura de la máquina.

* Todo proceso tiene una cierta cantidad de páginas que en cada instante son utilizadas frecuentemente.
_Si el proceso no dispone de suficientes marcos como para alojar dichas páginas, generará fallos de página con demasiada frecuencia_.
Esto afecta negativamente al rendimiento del sistema, por lo que es conveniente que el sistema asigne al número de marcos necesario para que eso no ocurra.

En general, si se va reduciendo el número de marcos asignados a un proceso, mucho antes de haber alcanzado el mínimo establecido por la arquitectura, el proceso dejará de ser útil debido a la elevada tasa de fallos de página, que será mayor cuantos menos marcos tenga asignados.
Cuando eso ocurre se dice que el proceso está _hiperpaginando_.

== Hiperpaginación

Se dice que _un proceso sufre de *hiperpaginación* cuando gasta más tiempo paginando que ejecutándos_e.

=== Causas de la hiperpaginación

En los primeros sistemas multiprogramados que implementaron la paginación bajo demanda era posible que se diera el siguiente caso:

. _El sistema operativo monitorizaba el uso de la CPU_.
Si el uso de la misma era bajo, se cargaban nuevos procesos desde la cola de entrada para aumentar el grado de multiprogramación.

. _Si un proceso necesitaba demasiada memoria, le podía quitar los marcos a otro_ puesto que se utilizaba un algoritmo de reemplazo global.
Esto podía ocasionar que aumentara la tasa de fallos de página del proceso que perdía los marcos.

. _Al aumentar los fallos de pagina el uso de la CPU decrecía_, por lo que el sistema operativo cargaba más procesos para aumentar el grado de multiprogramación y con ello el uso de la CPU.

. _Esto reducía la cantidad de memoria disponible para cada proceso_, lo que aumentaba la tasa de fallos de páginas que nuevamente reducía el uso de la CPU

. Este mecanismo iteraba hasta reducir considerablemente el rendimiento del sistema.

El fenómeno comentado se ilustra en la donde el uso de la CPU es trazado frente al número de procesos cargados en el sistema.
Cuando esto último aumenta el uso de la CPU aumenta hasta alcanzar un máximo.
Si el grado de multiprogramación supera dicho punto, el sistema comienza a hiperpaginar, por lo que el uso de la CPU disminuye bruscamente.
Por lo tanto, si el sistema está hiperpaginando, es necesario reducir el grado de multiprogramación con el objetivo de liberar memoria.

En los sistemas de tiempo compartido modernos ocurre algo parecido a lo descrito para los sistemas multiprogramados, aunque sin el efecto en cadena ocasionado por el intento del planificador de largo plazo de maximizar el uso de la CPU, ya que estos sistemas carecen de dicho planificador.
Sea como fuere, _en ambos casos los procesos hiperpaginarán si no se les asigna un número suficiente de marcos_.

=== Soluciones a la hiperpaginación

Para el problema de la hiperpaginación existen diversas soluciones:

* _Utiliza un algoritmo de reemplazo local_ pues de esta manera un proceso que hiperpagina no puede afectar a otro.
Sin embargo, el uso intensivo del dispositivo de intercambio podría afectar al rendimiento del sistema al aumentar el tiempo de acceso efectivo.

* _Proporcionar a un proceso tantos marcos como le hagan falta_.
Como ya hemos comentados en diversas ocasiones, para evitar la hiperpaginación es necesario asignar al procesos al menos un número mínimos de marcos, que a priori no es conocido.
Una de las estrategias que pretenden estimar dicho número es el *modelo de conjunto de trabajo*.

=== Modelo del conjunto de trabajo

Para entender el modelo de conjunto de trabajo es necesario comenzar definiendo el *modelo de localidad*.
El modelo de localidad establece que:

* _Una localidad es un conjunto de páginas que se utilizan juntas_.

* _Cuando un proceso se ejecuta se va moviendo de una localidad a otra_.

Por ejemplo, cuando se invoca una función se define una nueva localidad.
En esta localidad las referencias a la memoria se realizan al código de la función, a las variables locales de la misma y a algunas variables globales del programa.

Supongamos que proporcionamos a un proceso suficientes marcos como para alojar toda su localidad en un momento dado.
Entonces el proceso generará fallos de página hasta que todas las páginas de su localidad estén cargadas, pero después de eso no volverá a fallar hasta que no cambie a una nueva localidad.
Sin embargo _si damos al proceso menos marcos de los que necesita su localidad, éste hiperpaginará_.

_El *modelo de conjunto de trabajo* es una estrategia que permite obtener una aproximación de la localidad del programa_ y consiste en lo siguiente:

* _Definir el parámetro stem:[Delta] como el tamaño de la ventana del conjunto de trabajo_.

* _En un instante dado el conjunto de páginas presente en las stem:[Delta] referencias más recientes a la memoria se consideran el **conjunto de trabajo**_.

* Por lo tanto, _el *conjunto de trabajo* es una aproximación de localidad del programa_.

Por ejemplo, dada la siguiente lista de referencias a páginas en la memoria memoria:

// TODO: Arreglar esto.

si stem:[Delta = 10] referencias a la memoria, entonces el conjunto de trabajo en stem:[t_1] es stem:[{1, 2, 5, 6, 7}].
Mientras que en stem:[t_2] el conjunto de trabajo es stem:[{3, 4}].

Obviamente _la precisión del conjunto de trabajo como aproximación de la localidad del programa depende del parámetro stem:[Delta]_.
Por ejemplo:

* Si stem:[Delta] es muy pequeña, el conjunto de trabajo no cubría toda la localidad.

* Si stem:[Delta] es muy grande, el conjunto de trabajo se superpondrían a varias localidades.

=== Uso del conjunto del trabajo para evitar la hiperpaginación

El uso del conjunto de trabajo es bastante sencillo:

. _Se selecciona stem:[Delta]_.

. _El sistema operativo monitoriza el conjunto de trabajo de cada proceso y le asigna tantos marcos como páginas haya en el conjunto de trabajo_.

. _Si sobran suficientes marcos otro proceso puede ser iniciado —en el caso de los sistemas multiprogramados— o se puede destinar la memoria libre a otros usos_.

. _Si el tamaño del conjunto de trabajo D crece y excede el número de marcos disponibles, el sistema podría seleccionar un proceso para ser suspendido_.
Éste podrá volver a ser reiniciado más tarde.

Donde el tamaño del conjunto de trabajo stem:[D] es la suma del tamaño de los conjuntos de trabajo stem:[WSS^i] para cada proceso stem:[i]:

[stem]
++++
D=sum WSS_i
++++

y representa la demanda total de marcos.
Por eso _si stem:[D] es mayor que el número de marcos disponibles, habrá hiperpaginación_.

Este sencillo algoritmo anterior permite evitar la hiperpaginación.
Sin embargo, el problema está en como mover la ventana del conjunto de trabajo en cada referencia, con el fin de volver a calcular el conjunto de trabajo.
Una posible aproximación sería utilizar un temporizador que periódicamente invocase a una función encargada de examinar el bit de referencia de las páginas en la ventana _stem:[Delta]_.
Es de suponer que las páginas con el bit de referencia a 1 forman parte de la localidad del programa y por tanto serán el conjunto de trabajo a lo largo del siguiente periodo.

== Otras consideraciones

Ya hemos comentado que las principales decisiones que deben ser tomadas en el diseño de un sistema con paginación bajo demanda son la elección del algoritmo de reemplazo y la del de asignación de marcos de página.
Sin embargo hay otras consideraciones que deben ser tenidas en cuenta.

=== Prepaginado

_El *prepaginado* es una técnica que consiste en cargar múltiples páginas junto con la página demandada en cada fallo de página_.
Esas otras páginas se escogen especulativamente bajo la hipótesis de que van a ser necesitadas por el proceso en un corto espacio de tiempo, de manera que si la predicción es acertada la tasa de fallos de página se reduce significativamente.
Esta técnica puede ser utiliza, por ejemplo, en las siguiente situaciones:

* En la paginación bajo demanda pura, donde el sistema sabe de antemano que cuando se inicia un proceso siempre fallan las primeras páginas de código, por lo que son buenas candidatas para el prepaginado.

* En el acceso secuencial a archivos mapeados en memoria.
El sistema puede determinar que el acceso es de ese tipo tanto mediante el uso de técnicas heurísticas como mediante las indicaciones dadas por el proceso en la llamada al sistema con la que se abrió el archivo.
En cualquier caso, si el sistema determina que el acceso al archivo es secuencial, en cada fallo de página puede cargar tanto la página demanda como las siguientes en previsión de que vayan a ser utilizas por el proceso.

En general el único inconveniente del prepaginado es que debe ser ajustarlo para que el coste del mismo sea inferior al de servir los fallos de página.

=== Aplicaciones en modo RAW

_Algunas aplicaciones al acceder a sus datos a través de los mecanismos de memoria virtual del sistema operativo ofrecen peor rendimiento del que conseguirían si este mecanismo no existiera_.
El ejemplo típico son las bases de datos, que conocen sus necesidades de memoria y disco mejor que cualquier sistema operativo de propósito general, por lo que salen beneficiadas si implementan sus propios algoritmos de gestión de la memoria y de buffering de E/S.

_Muchos sistemas operativos modernos permiten que los programas que lo soliciten puedan acceder a los discos en modo raw_.
En el _modo raw_ no hay sistemas de archivos, ni paginación bajo demanda, ni bloqueo de archivos, ni prepaginación, ni nada; por lo que dichas aplicaciones deben implementar sus propios algoritmos de almacenamiento y gestión de la memoria.
Sin embargo, hay que tener en cuenta que la mayor parte de las aplicaciones siempre funcionan mejor utilizando los servicios convencionales ofrecidos por el sistema operativo.

=== Tamaño de las páginas

Como ya comentamos al estudiar el método básico de paginación (véase el <<_paginación>>), una decisión de diseño importante es escoger el tamaño adecuado para las páginas:

* *Con páginas grandes*:

    ** _Se consiguen menos fallos de páginas_.
    Por ejemplo, en un caso extremo un proceso de 100 KB solo podría generar un fallo de página si cada página es de 100 KB, pero puede generar 102400 fallos si cada pagina es de 1 byte.

    ** _Se consiguen tablas de páginas más pequeñas_.

    ** _La E/S para acceder al contenido de cada página requiere menos tiempo_.
    En general el tiempo de transferencia es proporcional a la cantidad de información transferida, lo que debería beneficiar a los sistemas con páginas de pequeño tamaño.
    Sin embargo la latencia y el tiempo requerido para posicionar la cabeza lectora de los discos es muy superior al tiempo de transferencias de datos, por lo que es más eficiente tener menos transferencias de mayor tamaño —como cuando se usan páginas de grandes— que más transferencias de menor tamaño —como cuando se usan páginas pequeñas—.

* *Con páginas pequeñas*:

    ** _Se consigue tener menos fragmentación interna_ y por tanto un mejor aprovechamiento de la memoria.

    ** _Teóricamente se obtiene mejor resolución para asignar y transferir al disco sólo la memoria que realmente necesitamos_.
    Esto a la larga debería redundar en menos memoria asignada y menos operaciones de E/S.

En la actualidad el tamaño de página más común es de 4KB en sistemas de 32 bits y 8 KB en los de 64 bits, ya que son adecuados para la mayor parte de las aplicaciones.
Sin embargo, _muchos sistemas modernos soportan el uso simultáneo de múltiples tamaños de página_.
Esto permite que la mayor parte de las aplicaciones utilicen el tamaño estándar, mientras las que hacen un uso intensivo de la memoria puedan utilizar páginas de mayor tamaño.
Por ejemplo, en la familia Intel x86 el tamaño estándar es de 4 KB, pero muchas bases de datos —como por ejemplo Oracle— y núcleos de sistema operativo —como por ejemplo Linux o Solaris— utilizan páginas de 4 MBfootnote:[ Es común que los núcleos de los sistemas operativos utilicen páginas de gran tamaño para alojar su código y sus datos.
De esta forma se minimiza el número de entradas de la TLB que utilizan, con el fin de disponer de más entradas libres para los procesos en ejecución.] cuando corren sobre dicha arquitectura.

=== Efecto de la estructura de los programas

_Los programas estructurados con un buena localidad de referencia pueden mejorar su rendimiento en los sistemas con paginación bajo demanda_.

Vamos a ilustrarlo con el siguiente ejemplo de un programa que inicializa a 0 un _array_ de 128 por 128 elementos.

[source, cpp]
----
int data[][] = new int[128][128];

for (int j = 0; j < 128; j++)
  for (int i = 0; i < 128; i++)
    data[i][j] = 0;
----

Un _array_ como el indicado es almacenado en filas:

[source, cpp]
----
data[0][0], data[0][1], ..., data[0][127]
data[1][0], data[1][1], ..., data[127][127]
----

De manera que si suponemos que el tamaño de cada página es de 128 palabras, en el mejor de los casos cada fila estará almacenada en una página.
Por lo tanto:

* Si el sistema le asigna 128 marcos o más, el proceso solo generará 128 fallos de página.

* Si el sistema operativo le asigna un solo marco, el proceso tendrá 16,384 fallos aproximadamente.

Sin embargo, el ejemplo sería diferente si el bucle interno del programa recorriera las columnas del _array_ y no las filas:

Pues se podrían a 0 primero todas las palabras de una misma página antes de empezar con la siguiente, reduciendo el número de fallos de página a 128 aunque el sistema operativo sólo asigne un marco al proceso.

Por lo tanto se puede concluir que:

* _La selección cuidadosa de las estructuras de datos y de programación pueden mejorar la localidad, reduciendo la tasa de fallos de páginas y el tamaño del conjunto de trabajo_.
Por ejemplo, las pilas tienen buena localidad puesto que el acceso siempre se realiza en lo alto de las mismas.
Sin embargo las tablas de dispersión, obviamente, están diseñadas para dispersar las referencias, lo que produce una mala localidad.

* _La elección del lenguaje de programación también puede tener efecto_.
En los lenguajes como C y {cpp} se utilizan punteros con frecuencia, lo que aleatoriza el acceso a la memoria empeorando la localidad de referencia.
Además algunos estudios indican que los lenguajes orientados a objetos tienden a tener peor localidad de referencia que los que no lo son.

* _El compilador y el cargador también pueden tener un efecto importante_:

    ** _Separando el código de los datos para permitir que las paginas de código pueda ser de sólo lectura_.
    Esto es interesante porque las paginas no modificadas no tienen que ser escritas antes de ser reemplazadas.

    ** _El compilador puede colocar las _funciones_ que se llaman entre sí en la misma página._

    ** _El cargador puede situar las _funciones_ _en la memoria _de _tal_ forma que _en lo posible no _crucen los bordes de las páginas_.

=== Interbloqueo de E/S

_Supongamos que un proceso solicita una operación de E/S sobre el contenido de alguna de las páginas de su espacio de direcciones y que la página es reemplazada después de que el proceso queda en espera pero antes de que la operación es realizada_.
En ese caso la operación de E/S se podría acabar realizando sobre una página que pertenece a un proceso diferente.
Para evitarlo existen diversas soluciones:

* _Se puede utilizar la memoria del núcleo como búfer en las operaciones de E/S_.
En una escritura esto obliga a la llamada al sistema a copiar los datos desde las páginas del proceso a la memoria del núcleo antes de solicitar la operación de E/S.
Mientras que en las operaciones de lectura sería justo al contrario.

* _Cada página puede tener un bit de bloqueo_ que se utiliza para indicar que páginas no pueden ser seleccionadas para reemplazo.

_Además los bits de bloqueo se pueden utilizar en otras muchas situaciones_:

* _Bloquear las páginas del núcleo para evitar que sean reemplazadas_.

* _Bloquear las páginas que acaban de ser cargadas_.
Esto evita que un proceso de mayor prioridad pueda reclamar el marco antes de que el proceso para el que se cargó la página sea reiniciado, desperdiciando el trabajo de cargarla y provocando un nuevo fallo de página.
Para implementarlo se puede poner el bit de bloqueo a 1 cuando la página se carga, volviéndolo a poner a 0 cuando el proceso es planificado por primera vez después del fallo de página que provocó la carga de la misma.

* _En los sistemas con tiempo real flexible se suele permitir que las tareas de tiempo real informen de cuales son las páginas más importantes con el fin de que sean bloqueadas para evitar que puedan ser reemplazadas_.
Para evitar riesgos, el sistema suele considerar estás solicitudes como _consejos de bloqueo_, de manera que es libre de descartar dichos consejos si el conjunto de marcos libres llega a ser demasiado pequeño o si un proceso concreto pide bloquear demasiadas páginas.

== Interfaz de gestión de la memoria

Gracias a la abstracción de las técnicas de memoria virtual —como la paginación bajo demanda— desde el punto de vista de los procesos en cualquier sistema moderno prácticamente sólo hace falta una llamada al sistema para gestionar su espacio de direcciones virtual.

En los sistemas POSIX esta llamada es {linux_mmap} y en Windows API es {win32_virtualalloc}, que use usan junto a sus opuestas {linux_munmap} y {win32_virtualfree}, respectivamente.

Ambas funciones permiten:

* Reservar una porción de espacio de direcciones virtual del proceso.
+
La llamada solo hace la reserva de un rango de direcciones para pueda ser utilizado por el proceso —es decir, que las páginas en ese rango sean válidas— siendo el componente de paginación bajo demanda el responsable de asignar la memoria física que lo respalda, cuando el proceso acceda a esas direcciones.

* Establecer permisos —lectura, escritura y ejecución—, opciones de compartición entre procesos, bloqueo de páginas en la memoria física, páginas de gran tamaño y otras opciones, en la región de memoria virtual a reservar.

[NOTE]
====
Además, en los sistemas POSIX, {linux_mmap} se utiliza también para mapear archivos en regiones del espacio de direcciones virtual.
Mientras que Windows API para esa función se utilizan llamadas diferentes, como hemos visto.
====

Ambas funciones ofrecen una buena cantidad de funcionalidades, pero operan a muy bajo nivel.
Por eso en ambas la página es la unidad mínima en la gestión de la memoria.
Es decir, las regiones reservadas del espacio de direcciones virtual siempre deben comienzar en un borde de página y su tamaño debe ser múltiplo del tamaño de página.

El problema es cómo compatibilizar eso con las necesidades reales de los programas, que durante su ejecución necesitan reservar y liberar constantemente memoria para pequeños elementos, como: _arrays_, cadenas de texto, estructuras u  objetos.
Para esos casos utilizar directamente {linux_mmap} o {win32_virtualalloc} no es una solución, puesto que la fragmentación interna conllevaría un importante derroche de recursos.

=== Anatomía del espacio de direcciones virtual del proceso

Los procesos pueden utilizar diversas ubicaciones dentro de su espacio de direcciones virtual para almacenar los datos que necesitan para su ejecución (véase la <<proceso_en_memoria_completo>>):

[[proceso_en_memoria_completo]]
.Anatomía de un proceso en memoria.
image::proceso_en_memoria_completo.svg[]

* La variables y constantes globales se almacenan en el *segmento de datos*(((segmento, datos))), que tiene tamaño fijo, ya que las dimensiones de estas variables se conocen de antemano en tiempo de compilación, al igual que ocurre con el código del programa.

* Las variables locales y los argumentos de las funciones se almacenan en la *((pila))*, junto con la direcciones de retorno para volver de las funciones.
+
Esta es la ubicación ideal para ellos, puesto que al retornar de una función, la pila se restablece al estado previo al que tenía cuando se invocó dicha función, haciendo que las variables locales y argumentos desaparezcan automáticamente.

* Las variables reservadas dinámicamente —por ejemplo, usando {clang_malloc}/{clang_free} en C o {cpp_new}/{cpp_delete} en {cpp} o Java— se almacenan en el *((montón))*, que no es más una región continua de memoria ubicada inmediatamente después del *segmento de datos* del proceso.

* En la región entre el *montón* y la *pila* se ubican los *archivos mapeados en memoria*, las regiones de *memoria compartida*, las *librerías de enlace dinámico*, las *pilas* de cada hilo —en procesos multihilo— y, en general, la memoria reservada con funciones como {linux_mmap} y {win32_virtualalloc}.

Cada lenguaje de programación debe proporcionar —a través de su *librería estándar*— un mecanismo en espacio de usuario adecuado para la gestión en tiempo de ejecución de la memoria del *montón* del proceso.
Para eso, cada lenguaje puede utilizar su propia implementación de dicho mecanismo o bien recurrir a la proporcionada por la *librería del sistema*.

Por ejemplo, en los sistemas POSIX la *librería del sistema* proporciona su propia implementación, accesible a través de las funciones {clang_malloc} y {clang_free}, que es utilizada directamente por los programas escritos en C.
Esta implementación hace uso de {linux_mmap}, pero ofrece mayor control sobre la cantidad de memoria que podemos reservar, como veremos en el <<_gestión_de_la_memoria_del_montón>>.

Otros lenguajes de programación tienen otras interfaces para gestionar la memoria, pero utilizan internamente las funciones {clang_malloc} y {clang_free} de la *librería del sistema*.
Sin embargo, este no es el caso ni de {cpp} ni de Java ni de algunos otros lenguajes.
En {cpp} los operadores {cpp_new} y {cpp_new} utilizan sus propios algoritmos de gestión de la memoria del *montón*, más optimizados que {clang_malloc} y {clang_free} para la creación y destrucción de objetos de cualquier tamaño de manera eficiente.

En Windows API ocurre algo similar.
La *librería del sistema* proporcionar su propia gestión de la memoria del *montón*, que es accesible para cualquier programa a través de las funciones {win32_heapalloc} y {win32_heapfree}, y que se implementa sobre {win32_virtualalloc}.
La *librería estándar* de C utiliza, a su vez, esas funciones para implementar {clang_malloc} y {clang_free}.
Lo mismo ocurre en otros lenguajes, aunque no en todos, ya que algunos optan por implementar algoritmos más eficientes para sus casos de uso directamente sobre {win32_virtualalloc}.

=== Gestión de la memoria del montón

Para ilustrar cómo se puede gestionar la memoria del *montón* utilizaremos como ejemplo el mecanismo empleado por la *librería del sistema* de los sistemas POSIX —accesible a través de las funciones {clang_malloc} y {clang_free}.
Sin embargo, es importante tener en cuenta que esta tarea se realiza de manera muy similar en las implementaciones de otros sistemas operativos y lenguajes de programación.

El funcionamiento básico de {clang_malloc} sigue las siguiente reglas:

. Cuando la memoria solicitada supera cierto umbral —128KB en sistemas GNU/Linux— es reservada directamente mediante la llamada al sistema {linux_mmap}.
Eso significa que las peticiones de gran tamaño realmente no consumen espacio del *montón*, si no que se reservan del hueco entre el *montón* y la *pila*.

. Cuando un proceso hace una petición de memoria dinámica espera que el espacio ofrecido sea continuo en el espacio de direcciones virtual, por lo que la memoria del *montón* se gestiona usando un algoritmo de *asignación contigua de memoria* (véase la <<_asignación_contigua_de_memoria>>)y, puesto que las peticiones pueden ser de tamaño variable, se utiliza con un esquema de *particionado dinámico*.
+
Es decir, que para las peticiones que no entran en el caso anterior, se busca en la tabla de huecos libres y ocupados del *montón* uno lo suficientemente como grande para atender la petición.
Se asigna el espacio solicitado y el resto sigue marcado como hueco libre.

[NOTE]
====
La estrategia más común de búsqueda es el *mejor ajuste*, utilizando algún tipo de estructura de datos que mantenga los huecos libres ordenados por tamaño, para encontrar el de tamaño adecuado rápidamente.
====

. Si no hay suficiente memoria libre contigua como para atender la petición, se utiliza la llamada al sistema {linux_mmap}, para extender el tamaño del *montón* reservando una nueva región separada —a veces llamada *((arena))*— y comenzar a repartirla.

[NOTE]
====
En aplicaciones pequeñas, algunas implementaciones intentan ampliar primero el espacio libre utilizando la llamada al sistema {linux_brk}, que sirve para extender el *montón* sobre la región adyacente no asignada del espacio de direcciones virtual del proceso.
Este es el caso de la implementación estándar de {clang_malloc} en GNU/Linux.

La llamada al sistema {linux_brk} ha sido eliminada del estándar POSIX, pero algunos sistemas la mantienen por compatibilidad hacia atrás, dado que era la forma en la que tradicionalmente se ampliaba la memoria del *montón* en los primeros UNIX.
En macOS esta llamada se emula con una región de 4 MiB reservada con *mmap* la primer vez que se utiliza.
====

=== Fragmentación

Las estrategia comentada sufre de *fragmentación interna*.
En las peticiones grandes {linux_mmap} reserva en múltiplos de tamaño de página, por lo que siempre se puede perder cierta cantidad, aunque pequeña en comparación al tamaño de la región reservada.
En las peticiones pequeñas, la memoria se asigna en múltiplos de una unidad mínima —por ejemplo, 16 o 32 bytes— por lo que también se puede perder cierta cantidad de memoria.

Además sufre de *fragmentación externa*, porque después de que el proceso lleva un tiempo en ejecución, liberando y reservando memoria, el espacio puede comenzar a quedar fraccionado en un gran número de pequeños huecos, obligando a la librería a buscar más espacio para el *montón*, aunque en suma haya suficiente espacio en los huecos libres.

Esto representa un reto para los desarrolladores de aplicaciones, que previsiblemente vayan a ejecutarse durante periodos muy largos de tiempo.
En esos casos es común optar por librerías externas, que implementen gestores de memoria que fragmenten menos la memoria, o soluciones basadas en alguna forma de referencias indirectas y recolección de basura, para ocasionalmente poder compactar la memoria del *montón*.
Esto último es lo que hace la máquina virtual de Java.
